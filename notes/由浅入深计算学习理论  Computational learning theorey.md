## 由浅入深计算学习理论  Computational learning theorey

## 1. 浅；大框架

1. **计算学习理论**：通过计算来进行学习的理论。分析学习任务的困难程度，为学习算法提供理论保障，并且根据分析结果知道算法设计。

2. **假设空间/模型**：是一组函数的集合，*f : X -> y* 。函数们映射的准确性却大不相同，统计学习的目标就是从假设空间中选取最优的模型，该模型能**尽可能准确的**将输入空间映射到输出空间。

3. **泛化误差/经验误差（也称训练误差）**

   ![泛化误差](../pics/泛化误差.PNG)![经验误差](../pics/经验误差.PNG)

4. VC维：描述假设空间的表示能力/复杂度。VC维越大说明模型的表示能力越强，同时要从中找到最优假设的难度也越大。

5. Rademacher复杂度：相比较于VC维，考虑了数据分布

6. 泛化误差界：量化在最坏的情况下，一个学习算法的表现。上面两种描述假设空间复杂度的方法，都可以定义泛化误差界，其中VC维的泛化误差界相较于Rademacher没有考虑数据分布的条件，VC维得到的泛化误差界比较松。两种界限后面再深入讨论。

7. 结构风险最小化 SRM：置信风险+经验风险，前者是分类器对未知样本分类时得到的误差，后者是对训练样本。机器学习的目标就是保证结构风险最小化，需要同时降低置信风险和经验风险，要达到这个目的，需要

> **计算学习理论**最基础的就是PAC Learning Theorey，概率近似正确地学习理论。它描述的是某学习算法能以多大概率地多接近目标概念，并且在该种情形下需要多少训练样例才能达到学习目的。（也就是样本复杂度）简单的用数学表示：
>
> ![pac可辨识](../pics/pac可辨识.PNG)
>
> ![PAC可学习](../pics/PAC可学习.PNG)
>
> 解读：某个函数和目标概念的差距小于等于*ϵ*的概率等于1-δ，并且通过推导可以知道其所需的样例数目m。

当然不止这些，现实的情况需要考虑很多其他的东西。





