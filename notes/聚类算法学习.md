## 聚类算法学习

```
聚类的一般步骤：
1. 特征选择
2. 相似性度量选择
3. 聚类算法
4. 结果验证
```

### 特征选择

避免冗余不正确的特征，实际测试聚类算法效果时，添加不正确的特征会影响聚类效果。

### 相似性度量

> 衡量两个观测点距离的度量

-  距离 (Minkovski  distance)
  -  曼哈顿距离 ( Manhattan distance ) L1 norm
  - 欧几里得距离 ( Eculidean distance ) L2 norm
  - 切比雪夫距离 (Chebyshev distance ) L∞ norm


- 相似系数 ( Similarity Parameter )
  - 余弦相似性
  - 应用非常广泛，主要优势是不受原线性变换影响，而且可以轻松的转换为距离。
  - 运算速度要比上面的举例法慢得多，特别是当数据的维度很高的时候。

### 聚类算法

- 划分聚类

  - K-means

  - > 想象将所有观测点聚集成不同的类别，使得同一类别内的观测点相似性高，不同类别观测点的相似性低。

    - 一般步骤：
      1. 通过尝试或先验知识猜测数据集大概存在k个类
      2. 随机选定k个观测点当作初始的聚簇中心
      3. 计算数据集内所有观测点到这k个聚簇中心的距离，将观测点标注成距离其最近的聚簇中心的类别
      4. 通过各观测点的标注重新计算各聚簇的中心位置
      5. 重复上面两步骤直到各个聚簇中心的位置不再发生变化；或者对于非凸的数据集，迭代次数达到预先设定的最大值
    - 优点：
      - 容易理解
      - 时间复杂度低
    - 缺点
      - 需要设定k值
      - 主要识别圆形簇，球型簇；无法处理环形或者其他形状
      - 不能处理具有离散特征的数据，只能处理连续型的特征数据（解决：k-modes）
      - 对噪声或异常值非常敏感（解决：k-medians）
      - 对凸数据的处理不好（解决：kernal k-means）

- 密度聚类

  - DBSCAN

  - > 简单来说就是画圈，需要决定圆圈的大小还有圆圈内最少要有几个点。

    - 一般步骤：
      1. 遍历数据集计算得所有的核心对象
      2. 随机选取一个核心对象，遍历该核心对象的ε邻域内的所有样本记录它们**密度可达**的所有其他样本点
      3. 从第一步骤的核心对象集合中删除已经在第二步骤遍历到的样本点
      4. 重复第二步骤，直到核心对象集合为空

